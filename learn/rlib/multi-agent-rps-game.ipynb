{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4de33d5",
   "metadata": {},
   "source": [
    "## Environment with Simultaneously Stepping Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bc18bf",
   "metadata": {},
   "source": [
    "A good and simple example for a multi-agent env, in which all agents always step simultaneously is the Rock-Paper-Scissors game, in which two agents have to play N moves altogether, each choosing between the actions “Rock”, “Paper”, or “Scissors”. After each move, the action choices are compared. Rock beats Scissors, Paper beats Rock, and Scissors beats Paper. The player winning the move receives a +1 reward, the losing player -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99296c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ray\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "import gymnasium as gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "492b13fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RockPaperScissors(MultiAgentEnv):\n",
    "    \"\"\"\n",
    "    Two player environment for the famous rock paper scissors game.\n",
    "    Both players always move simultaneously over a course of 10 timesteps in total.\n",
    "    The winner of each timestep receives reward of +1, the losing player -1.0.\n",
    "\n",
    "    The observation of each player is the last opponent action.\n",
    "    \"\"\"\n",
    "\n",
    "    ROCK = 0\n",
    "    PAPER = 1\n",
    "    SCISSORS = 2\n",
    "    LIZARD = 3\n",
    "    SPOCK = 4\n",
    "\n",
    "    PLAYER1 = \"player1\"\n",
    "    PLAYER2 = \"player2\"\n",
    "\n",
    "    WIN_MATRIX = {\n",
    "        (ROCK, ROCK): (0, 0),\n",
    "        (ROCK, PAPER): (-1, 1),\n",
    "        (ROCK, SCISSORS): (1, -1),\n",
    "        (PAPER, ROCK): (1, -1),\n",
    "        (PAPER, PAPER): (0, 0),\n",
    "        (PAPER, SCISSORS): (-1, 1),\n",
    "        (SCISSORS, ROCK): (-1, 1),\n",
    "        (SCISSORS, PAPER): (1, -1),\n",
    "        (SCISSORS, SCISSORS): (0, 0),\n",
    "    }\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # agents doesnt changes in episodes\n",
    "        self.agents = self.possible_agents = [self.PLAYER1, self.PLAYER2]\n",
    "\n",
    "        # The observations are always the last taken actions.\n",
    "        # Hence observation and action spaces are identical.\n",
    "        self.observation_spaces = self.action_spaces = {\n",
    "            self.PLAYER1: gym.spaces.Discrete(3),\n",
    "            self.PLAYER2: gym.spaces.Discrete(3),\n",
    "        }\n",
    "\n",
    "        self.last_move = None\n",
    "        self.num_moves = 0\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        self.num_moves = 0\n",
    "\n",
    "        observations = {\n",
    "            self.PLAYER1: 0,\n",
    "            self.PLAYER2: 0,\n",
    "        }\n",
    "        infos = {}\n",
    "\n",
    "        return observations, infos\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        self.num_moves += 1\n",
    "\n",
    "        move1 = action_dict[self.PLAYER1]\n",
    "        move2 = action_dict[self.PLAYER2]\n",
    "\n",
    "        observations = {self.PLAYER1: move2, self.PLAYER2: move1}\n",
    "\n",
    "        # Compute rewards for each player based on the win-matrix.\n",
    "        r1, r2 = self.WIN_MATRIX[move1, move2]\n",
    "        rewards = {self.PLAYER1: r1, self.PLAYER2: r2}\n",
    "\n",
    "        # Terminate and truncate the entire episode (for all agents) once 10 moves have been made.\n",
    "        terminateds = truncateds = {\"__all__\": self.num_moves >= 10}\n",
    "\n",
    "        infos = {}\n",
    "        return observations, rewards, terminateds, truncateds, infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfec01dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the environment with RLlib\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "def env_creator(config=None):\n",
    "    return RockPaperScissors(config)\n",
    "\n",
    "register_env('rps_multiagent', env_creator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "240f0f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ray.rllib.algorithms.ppo.ppo.PPOConfig at 0x149e790b3e0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "# PPOConfig setup for RLlib\n",
    "config = PPOConfig()\n",
    "config = config.environment(env=\"rps_multiagent\", env_config={})\n",
    "config = config.multi_agent(\n",
    "    policies={\n",
    "        \"player1\": (None, gym.spaces.Discrete(3), gym.spaces.Discrete(3), {}),\n",
    "        \"player2\": (None, gym.spaces.Discrete(3), gym.spaces.Discrete(3), {}),\n",
    "    },\n",
    "    policy_mapping_fn=lambda agent_id, *args, **kwargs: agent_id,\n",
    ")\n",
    "config.num_env_runners = 1\n",
    "\n",
    "config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d45b8fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'Version' and 'ValueError'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Build the algorithm\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m algo = \u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild_algo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Minor-Project-CF-MADRL\\myenv\\Lib\\site-packages\\ray\\rllib\\algorithms\\algorithm_config.py:1006\u001b[39m, in \u001b[36mAlgorithmConfig.build_algo\u001b[39m\u001b[34m(self, env, logger_creator, use_copy)\u001b[39m\n\u001b[32m   1003\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.algo_class, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   1004\u001b[39m     algo_class = get_trainable_cls(\u001b[38;5;28mself\u001b[39m.algo_class)\n\u001b[32m-> \u001b[39m\u001b[32m1006\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgo_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_copy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Minor-Project-CF-MADRL\\myenv\\Lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:469\u001b[39m, in \u001b[36mAlgorithm.__init__\u001b[39m\u001b[34m(self, config, env, logger_creator, **kwargs)\u001b[39m\n\u001b[32m    466\u001b[39m     config.environment(env)\n\u001b[32m    468\u001b[39m \u001b[38;5;66;03m# Validate and freeze our AlgorithmConfig object (no more changes possible).\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m469\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m config.freeze()\n\u001b[32m    472\u001b[39m \u001b[38;5;66;03m# Convert `env` provided in config into a concrete env creator callable, which\u001b[39;00m\n\u001b[32m    473\u001b[39m \u001b[38;5;66;03m# takes an EnvContext (config dict) as arg and returning an RLlib supported Env\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[38;5;66;03m# type (e.g. a gym.Env).\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Minor-Project-CF-MADRL\\myenv\\Lib\\site-packages\\ray\\rllib\\algorithms\\ppo\\ppo.py:287\u001b[39m, in \u001b[36mPPOConfig.validate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;129m@override\u001b[39m(AlgorithmConfig)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidate\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Call super's validation method.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    289\u001b[39m     \u001b[38;5;66;03m# Synchronous sampling, on-policy/PPO algos -> Check mismatches between\u001b[39;00m\n\u001b[32m    290\u001b[39m     \u001b[38;5;66;03m# `rollout_fragment_length` and `train_batch_size_per_learner` to avoid user\u001b[39;00m\n\u001b[32m    291\u001b[39m     \u001b[38;5;66;03m# confusion.\u001b[39;00m\n\u001b[32m    292\u001b[39m     \u001b[38;5;66;03m# TODO (sven): Make rollout_fragment_length a property and create a private\u001b[39;00m\n\u001b[32m    293\u001b[39m     \u001b[38;5;66;03m#  attribute to store (possibly) user provided value (or \"auto\") in. Deprecate\u001b[39;00m\n\u001b[32m    294\u001b[39m     \u001b[38;5;66;03m#  `self.get_rollout_fragment_length()`.\u001b[39;00m\n\u001b[32m    295\u001b[39m     \u001b[38;5;28mself\u001b[39m.validate_train_batch_size_vs_rollout_fragment_length()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Minor-Project-CF-MADRL\\myenv\\Lib\\site-packages\\ray\\rllib\\algorithms\\algorithm_config.py:962\u001b[39m, in \u001b[36mAlgorithmConfig.validate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    960\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_env_runner_settings()\n\u001b[32m    961\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_callbacks_settings()\n\u001b[32m--> \u001b[39m\u001b[32m962\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_framework_settings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_resources_settings()\n\u001b[32m    964\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_multi_agent_settings()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Minor-Project-CF-MADRL\\myenv\\Lib\\site-packages\\ray\\rllib\\algorithms\\algorithm_config.py:4948\u001b[39m, in \u001b[36mAlgorithmConfig._validate_framework_settings\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   4938\u001b[39m     \u001b[38;5;28mself\u001b[39m._value_error(\n\u001b[32m   4939\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot use `framework=tf` with the new API stack! Either switch to tf2\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4940\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m via `config.framework(\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtf2\u001b[39m\u001b[33m'\u001b[39m\u001b[33m)` OR disable the new API stack via \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4941\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`config.api_stack(enable_rl_module_and_learner=False)`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4942\u001b[39m     )\n\u001b[32m   4944\u001b[39m \u001b[38;5;66;03m# Check if torch framework supports torch.compile.\u001b[39;00m\n\u001b[32m   4945\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4946\u001b[39m     _torch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4947\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.framework_str == \u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m4948\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mversion\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_torch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__version__\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m<\u001b[49m\u001b[43m \u001b[49m\u001b[43mTORCH_COMPILE_REQUIRED_VERSION\u001b[49m\n\u001b[32m   4949\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.torch_compile_learner \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.torch_compile_worker)\n\u001b[32m   4950\u001b[39m ):\n\u001b[32m   4951\u001b[39m     \u001b[38;5;28mself\u001b[39m._value_error(\u001b[33m\"\u001b[39m\u001b[33mtorch.compile is only supported from torch 2.0.0\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4953\u001b[39m \u001b[38;5;66;03m# Make sure the Learner's torch-what-to-compile setting is supported.\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: '<' not supported between instances of 'Version' and 'ValueError'"
     ]
    }
   ],
   "source": [
    "# Build the algorithm\n",
    "\n",
    "algo = config.build_algo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c5bf13e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'algo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m results = []\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m20\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     result = \u001b[43malgo\u001b[49m.train()\n\u001b[32m      6\u001b[39m     learner_stats = result[\u001b[33m'\u001b[39m\u001b[33minfo\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mlearner\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'algo' is not defined"
     ]
    }
   ],
   "source": [
    "# Train the agents\n",
    "results = []\n",
    "\n",
    "for i in range(20):\n",
    "    result = algo.train()\n",
    "    learner_stats = result['info']['learner']\n",
    "    print(f\"Iteration {i+1}:\")\n",
    "    for agent, stats in learner_stats.items():\n",
    "        ls = stats['learner_stats']\n",
    "        print(\n",
    "            f\"  {agent}: total_loss={ls['total_loss']:.3f}, \"\n",
    "            f\"policy_loss={ls['policy_loss']:.3f}, \"\n",
    "            f\"vf_loss={ls['vf_loss']:.3f}, \"\n",
    "            f\"entropy={ls['entropy']:.3f}, \"\n",
    "            f\"kl={ls['kl']:.3f}\"\n",
    "        )\n",
    "    print(f\"  env_steps_sampled={result['info']['num_env_steps_sampled']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d437dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial observations:  {'player1': 0, 'player2': 0}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'algo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m agent_id \u001b[38;5;129;01min\u001b[39;00m obs:\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# Convert scalar obs to one-hot for the policy\u001b[39;00m\n\u001b[32m     13\u001b[39m     obs_onehot = np.eye(\u001b[32m3\u001b[39m)[obs[agent_id]]\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     policy = \u001b[43malgo\u001b[49m.get_policy(agent_id)\n\u001b[32m     15\u001b[39m     action = policy.compute_single_action(obs_onehot)\n\u001b[32m     16\u001b[39m     actions[agent_id] = \u001b[38;5;28mint\u001b[39m(action[\u001b[32m0\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'algo' is not defined"
     ]
    }
   ],
   "source": [
    "# Play a game using trained policies\n",
    "import numpy as np\n",
    "\n",
    "env = RockPaperScissors()\n",
    "obs, _ = env.reset()\n",
    "print(\"Initial observations: \", obs)\n",
    "\n",
    "total_rewards = {env.PLAYER1: 0, env.PLAYER2: 0}\n",
    "for step in range(10):\n",
    "    actions = {}\n",
    "    for agent_id in obs:\n",
    "        # Convert scalar obs to one-hot for the policy\n",
    "        obs_onehot = np.eye(3)[obs[agent_id]]\n",
    "        policy = algo.get_policy(agent_id)\n",
    "        action = policy.compute_single_action(obs_onehot)\n",
    "        actions[agent_id] = int(action[0])\n",
    "\n",
    "    # actions dict contains integer actions, which is correct for env.step\n",
    "    obs, rewards, terminateds, truncateds, infos = env.step(actions)\n",
    "    print(f\"Step {step+1}: Actions: {actions}, Rewards: {rewards}\")\n",
    "\n",
    "    for agent_id in rewards:\n",
    "        total_rewards[agent_id] += rewards[agent_id]\n",
    "\n",
    "    if terminateds.get(\"__all__\", False):\n",
    "        break\n",
    "\n",
    "print(f\"Total rewards: {total_rewards}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee5850d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
