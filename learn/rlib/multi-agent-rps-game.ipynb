{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4de33d5",
   "metadata": {},
   "source": [
    "## Environment with Simultaneously Stepping Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bc18bf",
   "metadata": {},
   "source": [
    "A good and simple example for a multi-agent env, in which all agents always step simultaneously is the Rock-Paper-Scissors game, in which two agents have to play N moves altogether, each choosing between the actions “Rock”, “Paper”, or “Scissors”. After each move, the action choices are compared. Rock beats Scissors, Paper beats Rock, and Scissors beats Paper. The player winning the move receives a +1 reward, the losing player -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99296c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ray\n",
      "  Downloading ray-2.53.0-cp312-cp312-manylinux2014_x86_64.whl.metadata (22 kB)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from ray) (8.3.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from ray) (3.20.2)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.12/dist-packages (from ray) (4.26.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray) (1.1.2)\n",
      "Requirement already satisfied: packaging>=24.2 in /usr/local/lib/python3.12/dist-packages (from ray) (25.0)\n",
      "Requirement already satisfied: protobuf>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from ray) (5.29.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from ray) (6.0.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from ray) (2.32.4)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray) (0.30.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->ray) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->ray) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->ray) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->ray) (2026.1.4)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from referencing>=0.28.4->jsonschema->ray) (4.15.0)\n",
      "Downloading ray-2.53.0-cp312-cp312-manylinux2014_x86_64.whl (72.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.4/72.4 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: ray\n",
      "Successfully installed ray-2.53.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-16 18:05:05,151\tWARNING compression.py:17 -- lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n"
     ]
    }
   ],
   "source": [
    "!pip install ray\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "import gymnasium as gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "492b13fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RockPaperScissors(MultiAgentEnv):\n",
    "    \"\"\"\n",
    "    Two player environment for the famous rock paper scissors game.\n",
    "    Both players always move simultaneously over a course of 10 timesteps in total.\n",
    "    The winner of each timestep receives reward of +1, the losing player -1.0.\n",
    "\n",
    "    The observation of each player is the last opponent action.\n",
    "    \"\"\"\n",
    "\n",
    "    ROCK = 0\n",
    "    PAPER = 1\n",
    "    SCISSORS = 2\n",
    "    LIZARD = 3\n",
    "    SPOCK = 4\n",
    "\n",
    "    PLAYER1 = \"player1\"\n",
    "    PLAYER2 = \"player2\"\n",
    "\n",
    "    WIN_MATRIX = {\n",
    "        (ROCK, ROCK): (0, 0),\n",
    "        (ROCK, PAPER): (-1, 1),\n",
    "        (ROCK, SCISSORS): (1, -1),\n",
    "        (PAPER, ROCK): (1, -1),\n",
    "        (PAPER, PAPER): (0, 0),\n",
    "        (PAPER, SCISSORS): (-1, 1),\n",
    "        (SCISSORS, ROCK): (-1, 1),\n",
    "        (SCISSORS, PAPER): (1, -1),\n",
    "        (SCISSORS, SCISSORS): (0, 0),\n",
    "    }\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # agents doesnt changes in episodes\n",
    "        self.agents = self.possible_agents = [self.PLAYER1, self.PLAYER2]\n",
    "\n",
    "        # The observations are always the last taken actions.\n",
    "        # Hence observation and action spaces are identical.\n",
    "        self.observation_spaces = self.action_spaces = {\n",
    "            self.PLAYER1: gym.spaces.Discrete(3),\n",
    "            self.PLAYER2: gym.spaces.Discrete(3),\n",
    "        }\n",
    "\n",
    "        self.last_move = None\n",
    "        self.num_moves = 0\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        self.num_moves = 0\n",
    "\n",
    "        observations = {\n",
    "            self.PLAYER1: 0,\n",
    "            self.PLAYER2: 0,\n",
    "        }\n",
    "        infos = {}\n",
    "\n",
    "        return observations, infos\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        self.num_moves += 1\n",
    "\n",
    "        move1 = action_dict[self.PLAYER1]\n",
    "        move2 = action_dict[self.PLAYER2]\n",
    "\n",
    "        observations = {self.PLAYER1: move2, self.PLAYER2: move1}\n",
    "\n",
    "        # Compute rewards for each player based on the win-matrix.\n",
    "        r1, r2 = self.WIN_MATRIX[move1, move2]\n",
    "        rewards = {self.PLAYER1: r1, self.PLAYER2: r2}\n",
    "\n",
    "        # Terminate and truncate the entire episode (for all agents) once 10 moves have been made.\n",
    "        terminateds = truncateds = {\"__all__\": self.num_moves >= 10}\n",
    "\n",
    "        infos = {}\n",
    "        return observations, rewards, terminateds, truncateds, infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfec01dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the environment with RLlib\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "def env_creator(config=None):\n",
    "    return RockPaperScissors(config)\n",
    "\n",
    "register_env('rps_multiagent', env_creator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "240f0f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ray.rllib.algorithms.ppo.ppo.PPOConfig at 0x78cfb05e60c0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "# PPOConfig setup for RLlib\n",
    "config = PPOConfig()\n",
    "config = config.environment(env=\"rps_multiagent\", env_config={})\n",
    "config = config.multi_agent(\n",
    "    policies={\n",
    "        \"player1\": (None, gym.spaces.Discrete(3), gym.spaces.Discrete(3), {}),\n",
    "        \"player2\": (None, gym.spaces.Discrete(3), gym.spaces.Discrete(3), {}),\n",
    "    },\n",
    "    policy_mapping_fn=lambda agent_id, *args, **kwargs: agent_id,\n",
    ")\n",
    "config.num_env_runners = 1\n",
    "\n",
    "config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d45b8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-16 18:05:10,033\tINFO tensorboardx.py:45 -- pip install \"ray[tune]\" to see TensorBoard files.\n",
      "2026-01-16 18:05:10,034\tWARNING unified.py:56 -- Could not instantiate TBXLogger: No module named 'tensorboardX'.\n",
      "2026-01-16 18:05:15,072\tINFO worker.py:2007 -- Started a local Ray instance.\n",
      "/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py:2046: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n",
      "\u001b[36m(pid=1835)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(pid=1835)\u001b[0m E0000 00:00:1768586723.584691    1835 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=1835)\u001b[0m E0000 00:00:1768586723.590571    1835 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=1835)\u001b[0m W0000 00:00:1768586723.607947    1835 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(pid=1835)\u001b[0m W0000 00:00:1768586723.607978    1835 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(pid=1835)\u001b[0m W0000 00:00:1768586723.607981    1835 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(pid=1835)\u001b[0m W0000 00:00:1768586723.607983    1835 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(pid=1835)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n",
      "\u001b[36m(pid=gcs_server)\u001b[0m [2026-01-16 18:05:40,165 E 1707 1707] (gcs_server) gcs_server.cc:303: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "2026-01-16 18:05:40,266\tINFO trainable.py:161 -- Trainable.setup took 30.231 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2026-01-16 18:05:40,267\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "# Build the algorithm\n",
    "\n",
    "algo = config.build_algo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c5bf13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2026-01-16 18:05:45,038 E 1796 1796] (raylet) main.cc:1032: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[36m(RolloutWorker pid=1835)\u001b[0m [2026-01-16 18:05:52,616 E 1835 1917] core_worker_process.cc:842: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "2026-01-16 18:05:54,161\tWARNING sgd.py:55 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "  player1: total_loss=2.695, policy_loss=-0.005, vf_loss=2.698, entropy=1.089, kl=0.009\n",
      "  player2: total_loss=2.696, policy_loss=-0.004, vf_loss=2.698, entropy=1.090, kl=0.009\n",
      "  env_steps_sampled=4000\n",
      "Iteration 2:\n",
      "  player1: total_loss=2.784, policy_loss=-0.007, vf_loss=2.789, entropy=1.079, kl=0.009\n",
      "  player2: total_loss=2.788, policy_loss=-0.005, vf_loss=2.791, entropy=1.087, kl=0.010\n",
      "  env_steps_sampled=8000\n",
      "Iteration 3:\n",
      "  player1: total_loss=2.740, policy_loss=-0.007, vf_loss=2.744, entropy=1.056, kl=0.013\n",
      "  player2: total_loss=2.740, policy_loss=-0.007, vf_loss=2.744, entropy=1.083, kl=0.015\n",
      "  env_steps_sampled=12000\n",
      "Iteration 4:\n",
      "  player1: total_loss=2.553, policy_loss=-0.003, vf_loss=2.554, entropy=1.066, kl=0.008\n",
      "  player2: total_loss=2.544, policy_loss=-0.015, vf_loss=2.555, entropy=1.040, kl=0.022\n",
      "  env_steps_sampled=16000\n",
      "Iteration 5:\n",
      "  player1: total_loss=2.836, policy_loss=-0.021, vf_loss=2.851, entropy=1.085, kl=0.032\n",
      "  player2: total_loss=2.849, policy_loss=-0.018, vf_loss=2.860, entropy=0.961, kl=0.023\n",
      "  env_steps_sampled=20000\n",
      "Iteration 6:\n",
      "  player1: total_loss=2.782, policy_loss=-0.017, vf_loss=2.793, entropy=1.064, kl=0.022\n",
      "  player2: total_loss=2.791, policy_loss=-0.004, vf_loss=2.793, entropy=0.949, kl=0.007\n",
      "  env_steps_sampled=24000\n",
      "Iteration 7:\n",
      "  player1: total_loss=2.803, policy_loss=-0.034, vf_loss=2.826, entropy=1.028, kl=0.024\n",
      "  player2: total_loss=2.817, policy_loss=-0.011, vf_loss=2.823, entropy=0.945, kl=0.010\n",
      "  env_steps_sampled=28000\n",
      "Iteration 8:\n",
      "  player1: total_loss=2.702, policy_loss=-0.018, vf_loss=2.711, entropy=0.975, kl=0.013\n",
      "  player2: total_loss=2.696, policy_loss=-0.012, vf_loss=2.704, entropy=1.002, kl=0.008\n",
      "  env_steps_sampled=32000\n",
      "Iteration 9:\n",
      "  player1: total_loss=2.786, policy_loss=-0.011, vf_loss=2.790, entropy=0.929, kl=0.009\n",
      "  player2: total_loss=2.772, policy_loss=-0.029, vf_loss=2.793, entropy=1.052, kl=0.020\n",
      "  env_steps_sampled=36000\n",
      "Iteration 10:\n",
      "  player1: total_loss=2.788, policy_loss=-0.001, vf_loss=2.788, entropy=0.937, kl=0.000\n",
      "  player2: total_loss=2.767, policy_loss=-0.032, vf_loss=2.790, entropy=1.059, kl=0.018\n",
      "  env_steps_sampled=40000\n",
      "Iteration 11:\n",
      "  player1: total_loss=2.632, policy_loss=-0.012, vf_loss=2.639, entropy=1.013, kl=0.013\n",
      "  player2: total_loss=2.602, policy_loss=-0.043, vf_loss=2.634, entropy=1.003, kl=0.025\n",
      "  env_steps_sampled=44000\n",
      "Iteration 12:\n",
      "  player1: total_loss=2.593, policy_loss=-0.024, vf_loss=2.608, entropy=1.068, kl=0.026\n",
      "  player2: total_loss=2.595, policy_loss=-0.014, vf_loss=2.603, entropy=0.953, kl=0.010\n",
      "  env_steps_sampled=48000\n",
      "Iteration 13:\n",
      "  player1: total_loss=2.714, policy_loss=-0.029, vf_loss=2.731, entropy=1.075, kl=0.023\n",
      "  player2: total_loss=2.724, policy_loss=-0.002, vf_loss=2.724, entropy=0.959, kl=0.002\n",
      "  env_steps_sampled=52000\n",
      "Iteration 14:\n",
      "  player1: total_loss=2.728, policy_loss=-0.018, vf_loss=2.736, entropy=1.056, kl=0.011\n",
      "  player2: total_loss=2.734, policy_loss=-0.005, vf_loss=2.737, entropy=0.979, kl=0.005\n",
      "  env_steps_sampled=56000\n",
      "Iteration 15:\n",
      "  player1: total_loss=2.756, policy_loss=-0.016, vf_loss=2.764, entropy=1.016, kl=0.011\n",
      "  player2: total_loss=2.741, policy_loss=-0.028, vf_loss=2.764, entropy=1.029, kl=0.031\n",
      "  env_steps_sampled=60000\n",
      "Iteration 16:\n",
      "  player1: total_loss=2.934, policy_loss=-0.010, vf_loss=2.939, entropy=0.990, kl=0.005\n",
      "  player2: total_loss=2.918, policy_loss=-0.030, vf_loss=2.941, entropy=1.038, kl=0.032\n",
      "  env_steps_sampled=64000\n",
      "Iteration 17:\n",
      "  player1: total_loss=2.844, policy_loss=-0.003, vf_loss=2.845, entropy=1.015, kl=0.003\n",
      "  player2: total_loss=2.839, policy_loss=-0.016, vf_loss=2.848, entropy=1.000, kl=0.019\n",
      "  env_steps_sampled=68000\n",
      "Iteration 18:\n",
      "  player1: total_loss=2.685, policy_loss=-0.014, vf_loss=2.693, entropy=1.041, kl=0.015\n",
      "  player2: total_loss=2.681, policy_loss=-0.015, vf_loss=2.691, entropy=0.948, kl=0.015\n",
      "  env_steps_sampled=72000\n",
      "Iteration 19:\n",
      "  player1: total_loss=2.825, policy_loss=-0.023, vf_loss=2.841, entropy=1.069, kl=0.018\n",
      "  player2: total_loss=2.822, policy_loss=-0.022, vf_loss=2.837, entropy=0.861, kl=0.019\n",
      "  env_steps_sampled=76000\n",
      "Iteration 20:\n",
      "  player1: total_loss=2.711, policy_loss=-0.034, vf_loss=2.738, entropy=1.063, kl=0.020\n",
      "  player2: total_loss=2.735, policy_loss=-0.005, vf_loss=2.737, entropy=0.838, kl=0.007\n",
      "  env_steps_sampled=80000\n"
     ]
    }
   ],
   "source": [
    "# Train the agents\n",
    "results = []\n",
    "\n",
    "for i in range(20):\n",
    "    result = algo.train()\n",
    "    learner_stats = result['info']['learner']\n",
    "    print(f\"Iteration {i+1}:\")\n",
    "    for agent, stats in learner_stats.items():\n",
    "        ls = stats['learner_stats']\n",
    "        print(\n",
    "            f\"  {agent}: total_loss={ls['total_loss']:.3f}, \"\n",
    "            f\"policy_loss={ls['policy_loss']:.3f}, \"\n",
    "            f\"vf_loss={ls['vf_loss']:.3f}, \"\n",
    "            f\"entropy={ls['entropy']:.3f}, \"\n",
    "            f\"kl={ls['kl']:.3f}\"\n",
    "        )\n",
    "    print(f\"  env_steps_sampled={result['info']['num_env_steps_sampled']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d437dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial observations:  {'player1': 0, 'player2': 0}\n",
      "Step 1: Actions: {'player1': 0, 'player2': 0}, Rewards: {'player1': 0, 'player2': 0}\n",
      "Step 2: Actions: {'player1': 2, 'player2': 1}, Rewards: {'player1': 1, 'player2': -1}\n",
      "Step 3: Actions: {'player1': 1, 'player2': 1}, Rewards: {'player1': 0, 'player2': 0}\n",
      "Step 4: Actions: {'player1': 1, 'player2': 0}, Rewards: {'player1': 1, 'player2': -1}\n",
      "Step 5: Actions: {'player1': 1, 'player2': 0}, Rewards: {'player1': 1, 'player2': -1}\n",
      "Step 6: Actions: {'player1': 2, 'player2': 0}, Rewards: {'player1': -1, 'player2': 1}\n",
      "Step 7: Actions: {'player1': 1, 'player2': 0}, Rewards: {'player1': 1, 'player2': -1}\n",
      "Step 8: Actions: {'player1': 1, 'player2': 0}, Rewards: {'player1': 1, 'player2': -1}\n",
      "Step 9: Actions: {'player1': 2, 'player2': 0}, Rewards: {'player1': -1, 'player2': 1}\n",
      "Step 10: Actions: {'player1': 0, 'player2': 0}, Rewards: {'player1': 0, 'player2': 0}\n",
      "Total rewards: {'player1': 3, 'player2': -3}\n"
     ]
    }
   ],
   "source": [
    "# Play a game using trained policies\n",
    "import numpy as np\n",
    "\n",
    "env = RockPaperScissors()\n",
    "obs, _ = env.reset()\n",
    "print(\"Initial observations: \", obs)\n",
    "\n",
    "total_rewards = {env.PLAYER1: 0, env.PLAYER2: 0}\n",
    "for step in range(10):\n",
    "    actions = {}\n",
    "    for agent_id in obs:\n",
    "        # Convert scalar obs to one-hot for the policy\n",
    "        obs_onehot = np.eye(3)[obs[agent_id]]\n",
    "        policy = algo.get_policy(agent_id)\n",
    "        action = policy.compute_single_action(obs_onehot)\n",
    "        actions[agent_id] = int(action[0])\n",
    "\n",
    "    # actions dict contains integer actions, which is correct for env.step\n",
    "    obs, rewards, terminateds, truncateds, infos = env.step(actions)\n",
    "    print(f\"Step {step+1}: Actions: {actions}, Rewards: {rewards}\")\n",
    "\n",
    "    for agent_id in rewards:\n",
    "        total_rewards[agent_id] += rewards[agent_id]\n",
    "\n",
    "    if terminateds.get(\"__all__\", False):\n",
    "        break\n",
    "\n",
    "print(f\"Total rewards: {total_rewards}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
